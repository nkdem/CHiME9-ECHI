{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Working with the CHiME-9 ECHI Data and Repository\n",
    "\n",
    "This notebook is designed to show you show to interact with the data and tools provided as part of the CHiME-9 ECHI Challenge. This script can either:\n",
    "* Be used as part of the CHiME-9 ECHI repo (i.e. from `CHiME9-ECHI/Quickstart.ipynb`)\n",
    "* As a standalone script which will clone the CHiME-9 ECHI repo into the current working directory (better for using with Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "## Setting up your environment\n",
    "\n",
    "This will install a subset of the dependencies required for the full CHiME-9 ECHI package, and add the required modules to your path.\n",
    "\n",
    "If this notebook is being used on Google Colab/has been downloaded on its own, the `CHiME9-ECHI` repository will be cloned into the curent working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68462,
     "status": "ok",
     "timestamp": 1758874525054,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "2",
    "outputId": "4e8644c5-1d9a-46c9-9116-06b9be9f553f"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "\n",
    "packages = {\n",
    "    \"stoi\": \"pystoi\",\n",
    "    \"soundfile\": \"soundfile\",\n",
    "    \"soxr\": \"soxr\",\n",
    "    \"torch\": \"torch\",\n",
    "    \"ipywidgets\": \"ipywidgets\",\n",
    "    \"pysepm\": \"https://github.com/schmiph2/pysepm/archive/master.zip\",\n",
    "    \"gdown\": \"gdown\",\n",
    "}\n",
    "failures = []\n",
    "\n",
    "for imp_name, pkg in packages.items():\n",
    "    %pip install {pkg}\n",
    "\n",
    "    if \"pysepm\" in pkg:\n",
    "        pkg = \"pysepm\"\n",
    "\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        failures.append(imp_name)\n",
    "\n",
    "if failures:\n",
    "    print(f\"❌ Failed to import: {', '.join(failures)}\")\n",
    "\n",
    "else:\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    clear_output()\n",
    "    print(\"✅ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758874559820,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "3",
    "outputId": "f81b9a1b-4426-478c-b150-700445a65496"
   },
   "outputs": [],
   "source": [
    "# Clone the public repo and add to path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "if cwd.stem == \"CHiME9-ECHI\":\n",
    "    # Using this script as part of the main repository, locally on Jupyter\n",
    "    sys.path.append(\"src\")\n",
    "    repo_root = Path(\".\")\n",
    "else:\n",
    "    # Using as a standalone script\n",
    "    if \"CHiME9-ECHI\" not in os.listdir(cwd):\n",
    "        !git clone https://github.com/schmiph2/CHiME9-ECHI.git\n",
    "    if str(cwd / \"CHiME9-ECHI\") not in sys.path:\n",
    "        print(\"Adding CHiME9-ECHI to sys.path\")\n",
    "        sys.path.append(str(cwd / \"CHiME9-ECHI/src\"))\n",
    "    repo_root = cwd / Path(\"CHiME9-ECHI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "source": [
    "## Downloading the demo data\n",
    "\n",
    "A few part-sessions from the development set have been saved into Google Drive\n",
    "for download. The `tar.gz` file to download is 1.5GB, and when unpacked the data\n",
    "uses 2.7GB of disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1758874561503,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "5",
    "outputId": "f0a9fea1-18e3-4a5b-fb1f-6edf2a222f4d"
   },
   "outputs": [],
   "source": [
    "# Download the demo data\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_root = repo_root / \"data\"\n",
    "chime9echi_root = data_root / \"chime9_echi.demo\"\n",
    "targz_file = data_root / \"chime9_echi.demo.tar.gz\"\n",
    "\n",
    "\n",
    "if chime9echi_root.exists():\n",
    "    print(\"Data already downloaded!\")\n",
    "else:\n",
    "    if not targz_file.exists():\n",
    "        print(\"No tar.gz found. Downloading...\")\n",
    "        !gdown --fuzzy \"https://drive.google.com/file/d/1nDCoLr4NA-CAeHEPsylerQkUeJf_hnCW/view?usp=sharing\" -O {targz_file}\n",
    "    print(\"Unzipping demo data...\")\n",
    "    !tar -xvzf {targz_file} -C {data_root}\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    clear_output()\n",
    "    print(\"✅ Data downloaded and unzipped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758874563897,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "6"
   },
   "outputs": [],
   "source": [
    "noisy_ftemplate = str(chime9echi_root / \"{device}/dev/{session}.{device}.wav\")\n",
    "ref_ftemplate = str(chime9echi_root / \"ref/dev/{session}.{device}.{pid}.wav\")\n",
    "rainbow_ftemplate = str(chime9echi_root / \"participant/dev/{pid}.wav\")\n",
    "segments_ftemplate = str(\n",
    "    chime9echi_root / \"metadata/ref/dev/{session}.{device}.{pid}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "7"
   },
   "source": [
    "These variables correspond to:\n",
    "* `noisy_ftemplate` is the path to the noisy audio, requiring the `session` and `device` to be specified\n",
    "* `ref_ftemplate` points to the reference conversational speech, requiring `session`, `device` and `pid`\n",
    "* `rainbow_ftemplate` refers to the clean speech samples for each participant, requiring `pid`\n",
    "* `segments_ftemplate` gives the path to the CSV file containing the speech segments for each participant, requiring `session`, `device` and `pid`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "## Using the data\n",
    "\n",
    "Now that we have downloaded some data, let's load some of it in and have a look. First, we need to define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1758874565584,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "9"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import soxr\n",
    "from IPython.display import Audio\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "def load_session_audio(session, device, target, segment):\n",
    "    \"\"\"Load the noisy and reference audio for a session\"\"\"\n",
    "    audio = {}\n",
    "\n",
    "    noisy_fpath = noisy_ftemplate.format(device=device, session=session)\n",
    "\n",
    "    model_fs = 16000\n",
    "\n",
    "    noisy_audio, fs = sf.read(noisy_fpath)\n",
    "\n",
    "    clip_start = int(segment[\"start\"] * fs / model_fs)\n",
    "    clip_end = int(segment[\"end\"] * fs / model_fs)\n",
    "\n",
    "    noisy_audio = noisy_audio[clip_start:clip_end]\n",
    "    noisy_audio = soxr.resample(noisy_audio, fs, model_fs)\n",
    "\n",
    "    audio[\"noisy\"] = noisy_audio\n",
    "\n",
    "    ref_file = ref_ftemplate.format(session=session, device=device, pid=target)\n",
    "    ref_audio, _ = sf.read(ref_file)\n",
    "    audio[\"ref\"] = ref_audio[segment[\"start\"] : segment[\"end\"]]\n",
    "\n",
    "    rainbow_file = rainbow_ftemplate.format(pid=target)\n",
    "    rainbow_audio, fs = sf.read(rainbow_file)\n",
    "    rainbow_audio = soxr.resample(rainbow_audio, fs, model_fs)\n",
    "    audio[\"rainbow\"] = rainbow_audio\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def labeled_audio(audio_dict, rate=16000):\n",
    "    \"\"\"Return an HBox with a label and audio player side by side.\"\"\"\n",
    "\n",
    "    players = []\n",
    "    for label, data in audio_dict.items():\n",
    "        label_widget = widgets.Label(value=label, layout=widgets.Layout(width=\"100px\"))\n",
    "        audio_widget = Audio(data.T, rate=16000)._repr_html_()\n",
    "        audio_widget = widgets.HTML(value=audio_widget)\n",
    "        players.append(widgets.HBox([label_widget, audio_widget]))\n",
    "    return widgets.VBox(players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "source": [
    "Now let's load in the metadata file and see what information we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758874567371,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "11",
    "outputId": "7e984880-8410-4e2b-fa80-c50ed2eaae65"
   },
   "outputs": [],
   "source": [
    "# Load in the session information\n",
    "import csv\n",
    "import json\n",
    "\n",
    "with open(chime9echi_root / \"metadata/sessions.dev.csv\", \"r\") as file:\n",
    "    sessions = list(csv.DictReader(file))\n",
    "\n",
    "sessions = {s[\"session\"]: s for s in sessions}\n",
    "print(json.dumps(sessions, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "source": [
    "This gives us information about the sessions that have been downloaded, including:\n",
    "* Which participants were present in the session\n",
    "* Which device was in which position. For example, in `dev_10`, the Aria glasses were worn in `pos3` by `p182`, and the hearing aids were worn in `pos4` by `P180`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758874570249,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "13"
   },
   "outputs": [],
   "source": [
    "session = \"dev_10\"\n",
    "session_info = sessions[session]\n",
    "device = \"aria\"\n",
    "all_targets = [\n",
    "    session_info[f\"pos{i}\"]\n",
    "    for i in range(1, 5)\n",
    "    if str(i) != session_info[f\"{device}_pos\"]\n",
    "]\n",
    "target = all_targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "source": [
    "As well as this session metadata, we also have reference segments, indicating when each participant is speaking, stored in CSV files. These files are separated by session, device and participant.\n",
    "\n",
    "**NOTE** The reference segments are slightly different on the Aria glasses compared to the hearing aids. This is because they are always worn by different people in the session, so the propogation delays for each person's speech is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1758874571881,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "15",
    "outputId": "81a31569-5da7-4cbd-d95c-c76949bb2df7"
   },
   "outputs": [],
   "source": [
    "segments_file = segments_ftemplate.format(session=session, device=device, pid=target)\n",
    "with open(segments_file, \"r\") as file:\n",
    "    segments = [\n",
    "        {a: int(b) for a, b in seg.items()}\n",
    "        for seg in csv.DictReader(file, fieldnames=[\"index\", \"start\", \"end\"])\n",
    "    ]\n",
    "\n",
    "print(json.dumps(segments[:3], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "16"
   },
   "source": [
    "The time stamps for these segments correspond the the start/end sample when using a sampling frequency of 16 kHz. So the first segments starts at sample 407679, which is 407679/16000=25.48 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758874575221,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "17"
   },
   "outputs": [],
   "source": [
    "segment = segments[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "18"
   },
   "source": [
    "For all systems, the only input that can be provided to the model is:\n",
    "* The noisy audio from either the Aria glasses **OR** the hearing aids (never both at once)\n",
    "* The clean speech sample of the target(s) voice\n",
    "\n",
    "Then a reference signal containing the clean, conversation speech if also provided to be used as a training target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221,
     "referenced_widgets": [
      "3b07622d0d34420c8ef93f174501644b",
      "5524520b399b41bf88cbb2b5c345668d",
      "e2d189c4ffa3495b90b8976d87b24341",
      "42fa0b3c48f14709a6b1b36f6df88ddb",
      "5c34412cb7954686a5ff2f966793dc88",
      "884d45520fd64728a3b9e2644cb11a83",
      "f28cc9831ac948988ad03067c2bae186",
      "8195522add9f4d47986bc9bcac0a12a6",
      "ae02734a5b2f407d981181c3c5a17675",
      "4fbf97eac1144b90acf5597906d259ad",
      "dc8fc9cb4c054a24968d913b073eda2f",
      "f115de3439894135a8ce4e1d06376fa8",
      "551812323a30414ba1a14d596d0090d7",
      "7339d41f546342dc86d423d89166cb9e",
      "cb5eaa8980c1400bb589af555967e485",
      "fa97aee6fc384821b1397daa9470c229",
      "fe26d843cdc54168b0f8bc76705d5c3e",
      "a46d48c661ed41e486e08f99f7a522cd",
      "752946d2ddf049249818a0ff334a425c",
      "4e663216aa7a4d909c003d9e7886d8a1",
      "ce196d387e7148d69c08b03cdf7c6b36",
      "a54c7da43e2341c884e123f2c9be0142",
      "a5ef2b23d6f34a0b800eca7f92178153",
      "9bae9bda9bde47d68f110291aab75ba6",
      "9ed8c631dd0940b9beb2c25ba584c8b1",
      "89c87708b34c472c86ca05dafd49eb2a"
     ]
    },
    "executionInfo": {
     "elapsed": 4489,
     "status": "ok",
     "timestamp": 1758874581370,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "19",
    "outputId": "20b0bd12-61af-4ad1-82a4-270845e61cfd"
   },
   "outputs": [],
   "source": [
    "audio = load_session_audio(session, device, target, segment)\n",
    "\n",
    "labeled_audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "20"
   },
   "source": [
    "## Enhancing the speech\n",
    "\n",
    "Now we've loaded the audio, we can use the baseline system to extract the target\n",
    "speaker's speech from the noisy audio.\n",
    "\n",
    "The config file and checkpoints for the baseline system are stored in\n",
    "`CHiME9-ECHI/checkpoints`. The config can be loaded using `omegaconf`, and the\n",
    "baseline model can be loaded from the checkpoint using the `get_model` function\n",
    "provided in the repo.\n",
    "\n",
    "We also load in a STFT wrapper, based on the requirements of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1758874631823,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "21"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from shared.core_utils import get_model\n",
    "from shared.signal_utils import STFTWrapper\n",
    "\n",
    "cfg = OmegaConf.load(repo_root / f\"checkpoints/{device}_config.yaml\")\n",
    "\n",
    "stft = STFTWrapper(**cfg.input.stft)\n",
    "model = get_model(cfg, repo_root / f\"checkpoints/{device}_baseline.pt\")  # type: ignore\n",
    "model.eval()\n",
    "\n",
    "noisy_audio = torch.from_numpy(audio[\"noisy\"].T)\n",
    "rainbow_audio = torch.from_numpy(audio[\"rainbow\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "22"
   },
   "source": [
    "The baseline system takes three inputs:\n",
    "* The STFT of the noisy audio, in shape `[batch, channels, time, freqs]`\n",
    "* The STFT of the rainbow audio, in shape `[batch, time, freqs]`\n",
    "* The lengths of the rainbow audio, in shape `[batch]`\n",
    "    * When the batch size is greater than one, the rainbow passages may be\n",
    "    zero-padded to match the lengths\n",
    "    * Providing the rainbow lengths means only the speech is processed, and not\n",
    "    the zero-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "executionInfo": {
     "elapsed": 26273,
     "status": "ok",
     "timestamp": 1758874660081,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "23"
   },
   "outputs": [],
   "source": [
    "noisy_stft = stft(noisy_audio).unsqueeze(0).to(torch.float)\n",
    "rainbow_stft = stft(rainbow_audio).to(torch.float)\n",
    "rainbow_len = torch.tensor([rainbow_stft.shape[2]]).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_stft = model(noisy_stft, rainbow_stft, rainbow_len).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221,
     "referenced_widgets": [
      "a642f00a7d734324bf194f5349b8ef3c",
      "0f1ffd84958e40cfb8727c75ffa3e9b2",
      "3d64a0f7a0ed47768c7a052f146cfba4",
      "5ee031d9387e4409acd3049342358b0d",
      "125a93c594f049859fa2434b5b5b44bc",
      "0dbe879894d3427886f7efdfb4e24f50",
      "92354a71f4bb40afa666ee93d8e8f8ab",
      "fbe80a9d1cbf4d88a0b79595ffebd62a",
      "e92638e688d94ab982463e2986654b50",
      "bcf215faba174fe684a2be4057484c5c",
      "594b180c7dff4993b6cbe70855bf378e",
      "8354b6968d7b4f3d9d013d3f70896f0f",
      "5f9e42d6766e41fca0364026943d988e",
      "e49219994fcc49e19b8815effe506855",
      "82a2ee23ca954ee2b563485ff8892589",
      "2f0c65f0f2a240dfbd83abd6a349ee60",
      "2f0bfe47f3074ed588071aae5dc0d584",
      "f5900afdb0e14928abb13e3178e96ac5",
      "66fd5f33e70247c9bc1de9761df77fab",
      "9066053600af4f3cb777856ffea2fc12",
      "f3a65902ee3f4343ae1491e2293c11b7",
      "1d17b537e28248eeaf76afa7ad49a33f",
      "cc4fe0221b0b49be9698943aaae95a46",
      "419986ba6c6a487a863618b62c15b0cc",
      "66e03cd4954e4ff29412e685ca7ab716",
      "dee4b21e0c9743a58343ee296dc270e9"
     ]
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1758874664294,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "24",
    "outputId": "60189341-cea6-49de-bac0-8998f2677052"
   },
   "outputs": [],
   "source": [
    "output_audio = stft.inverse(output_stft).squeeze(0, 1).detach().numpy()\n",
    "\n",
    "audio = {\"noisy\": audio[\"noisy\"], \"output\": output_audio, \"ref\": audio[\"ref\"]}\n",
    "labeled_audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "25"
   },
   "source": [
    "## Evaluating the enhanced audio\n",
    "\n",
    "The `CHiME9-ECHI` repository relies on the\n",
    "[WavLab Versa toolkit](https://github.com/wavlab-speech/versa) to compute the\n",
    "scores from a wide variety of metrics. For simplicity, in this notebook we will\n",
    "use a handful of these metrics directly:\n",
    "* STOI: Short-Time Objective Intelligibility which assesses how easy a signal is\n",
    "to understand (from [pystoi](https://github.com/mpariente/pystoi))\n",
    "* FWSegSNR: Frequency-WEighted Segmental SNR, which scores the SNR of the signal\n",
    "with a weighting applied to the frequencies to more accurately reflect human\n",
    "hearing (from [pysepm](https://github.com/schmiph2/pysepm))\n",
    "* The Composite metrics, designed to evaluate the quality of the signal\n",
    "(Csig), intrusiveness of the background (CBak), and the overall quality (Covl),\n",
    "(also from [pysepm](https://github.com/schmiph2/pysepm))\n",
    "\n",
    "Note that these metrics are only to be used as a guide, and final evaluation of\n",
    "system will be down to listening tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8057,
     "status": "ok",
     "timestamp": 1758874681233,
     "user": {
      "displayName": "Robbie Sutherland",
      "userId": "02585878721229379090"
     },
     "user_tz": -60
    },
    "id": "26",
    "outputId": "d2c0f925-a535-4b75-e424-8a382561efbd"
   },
   "outputs": [],
   "source": [
    "from pysepm.qualityMeasures import composite, fwSNRseg\n",
    "from pystoi import stoi\n",
    "\n",
    "if device == \"aria\":\n",
    "    noisy_mono = noisy_audio[3, :]  # The fourth channel from the Aria glasses\n",
    "else:\n",
    "    noisy_mono = noisy_audio[:2, :].sum(\n",
    "        dim=0\n",
    "    )  # Sum of the left-front and right-fron channels of the hearing aids\n",
    "\n",
    "noisy_mono = noisy_mono.detach().cpu().numpy()\n",
    "ref_audio = audio[\"ref\"]\n",
    "\n",
    "scores = {}\n",
    "scores[\"STOI\"] = [\n",
    "    stoi(ref_audio, noisy_mono, 16000),\n",
    "    stoi(ref_audio, output_audio, 16000),\n",
    "]\n",
    "scores[\"FWSegSNR\"] = [\n",
    "    fwSNRseg(ref_audio, noisy_mono, 16000),\n",
    "    fwSNRseg(ref_audio, output_audio, 16000),\n",
    "]\n",
    "csig_noisy, cbak_noisy, covl_noisy = composite(ref_audio, noisy_mono, 16000)\n",
    "csig_output, cbak_output, covl_output = composite(ref_audio, output_audio, 16000)\n",
    "\n",
    "scores[\"Csig\"] = [csig_noisy, csig_output]\n",
    "scores[\"Cbak\"] = [cbak_noisy, cbak_output]\n",
    "scores[\"Covl\"] = [covl_noisy, covl_output]\n",
    "\n",
    "print(\"{:<10}{:<10}{:<10}\".format(\"Metric\", \"Noisy\", \"Baseline\"))\n",
    "\n",
    "for metric, pair in scores.items():\n",
    "    print(\"{:<10}{:<10.2f}{:<10.2f}\".format(metric, *pair))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
